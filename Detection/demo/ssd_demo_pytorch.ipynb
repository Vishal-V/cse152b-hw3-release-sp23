{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separate-beach",
   "metadata": {
    "id": "separate-beach"
   },
   "source": [
    "# SSD300\n",
    "\n",
    "Based on Nvidia's tutorial/demo. It may be better to run this on Google Colab with a GPU instance.\n",
    "\n",
    "**Single Shot MultiBox Detector model for object detection**\n",
    "\n",
    "### Model Description and modifications\n",
    "\n",
    "This SSD300 model is based on the\n",
    "[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) paper, which\n",
    "describes SSD as â€œa method for detecting objects in images using a single deep neural network\".\n",
    "The input size is fixed to 300x300.\n",
    "\n",
    "The main difference between this model and the one described in the paper is in the backbone.\n",
    "Specifically, the VGG model is obsolete and is replaced by the ResNet-50 model.\n",
    "\n",
    "From the\n",
    "[Speed/accuracy trade-offs for modern convolutional object detectors](https://arxiv.org/abs/1611.10012)\n",
    "paper, the following enhancements were made to the backbone:\n",
    "*   The conv5_x, avgpool, fc and softmax layers were removed from the original classification model.\n",
    "*   All strides in conv4_x are set to 1x1.\n",
    "\n",
    "The backbone is followed by 5 additional convolutional layers.\n",
    "In addition to the convolutional layers, we attached 6 detection heads:\n",
    "*   The first detection head is attached to the last conv4_x layer.\n",
    "*   The other five detection heads are attached to the corresponding 5 additional layers.\n",
    "\n",
    "Detector heads are similar to the ones referenced in the paper, however, they are enhanced by additional BatchNorm layers after each convolution.\n",
    "\n",
    "### Example\n",
    "\n",
    "In the example demo below we will use the pretrained SSD model to detect objects in sample images and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-shame",
   "metadata": {
    "id": "cleared-shame"
   },
   "outputs": [],
   "source": [
    "! pip install numpy scipy scikit-image matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-lender",
   "metadata": {
    "id": "graphic-lender"
   },
   "source": [
    "Load an SSD model pretrained on COCO dataset, as well as a set of utility methods for convenient and comprehensive formatting of input and output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-greensboro",
   "metadata": {
    "id": "single-greensboro"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd')\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd_processing_utils')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-activation",
   "metadata": {
    "id": "auburn-activation"
   },
   "source": [
    "Now, prepare the loaded model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-compilation",
   "metadata": {
    "id": "adjustable-compilation"
   },
   "outputs": [],
   "source": [
    "ssd_model.to('cuda')\n",
    "ssd_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-guitar",
   "metadata": {
    "id": "affecting-guitar"
   },
   "source": [
    "As per HW3, please upload a few images from Pascal VOC and from other sources. Note: Do not modify the Pascal VOC dataset on Datahub (if you wish, you may copy some images to the appropriate folder/Colab and visualize the outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comic-japan",
   "metadata": {
    "id": "comic-japan"
   },
   "outputs": [],
   "source": [
    "uris = [\n",
    "    # Path to your images here. Note that the correct path be provided if you are running this on Google Colab. \n",
    "    # Use the side bar to upload images and use te appropriate file paths.\n",
    "    # Eg:\n",
    "    # '../../public/datasets/img_1.jpg',\n",
    "    # '../../public/datasets/img_2.jpg'\n",
    "    # '../../public/datasets/img_3.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-joshua",
   "metadata": {
    "id": "unique-joshua"
   },
   "source": [
    "Format the images to comply with the network input and convert them to tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-functionality",
   "metadata": {
    "id": "similar-functionality"
   },
   "outputs": [],
   "source": [
    "inputs = [utils.prepare_input(uri) for uri in uris]\n",
    "tensor = utils.prepare_tensor(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-driving",
   "metadata": {
    "id": "neither-driving"
   },
   "source": [
    "Run the SSD network to perform object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-probe",
   "metadata": {
    "id": "latin-probe"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    detections_batch = ssd_model(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-kuwait",
   "metadata": {
    "id": "horizontal-kuwait"
   },
   "source": [
    "By default, raw output from SSD network per input image contains\n",
    "8732 boxes with localization and class probability distribution.\n",
    "Let's filter this output to only get reasonable detections (confidence>40%) in a more comprehensive format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-bubble",
   "metadata": {
    "id": "early-bubble"
   },
   "outputs": [],
   "source": [
    "results_per_input = utils.decode_results(detections_batch)\n",
    "best_results_per_input = [utils.pick_best(results, 0.40) for results in results_per_input]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-ceremony",
   "metadata": {
    "id": "catholic-ceremony"
   },
   "source": [
    "This model was trained on COCO dataset, which we need to access in order to translate class IDs into object names.\n",
    "For the first time, downloading annotations may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-navigator",
   "metadata": {
    "id": "analyzed-navigator"
   },
   "outputs": [],
   "source": [
    "classes_to_labels = utils.get_coco_object_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-terminology",
   "metadata": {
    "id": "spread-terminology"
   },
   "source": [
    "Finally, visualize our detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-documentation",
   "metadata": {
    "id": "solar-documentation"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "for image_idx in range(len(best_results_per_input)):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Show original, denormalized image...\n",
    "    image = inputs[image_idx] / 2 + 0.5\n",
    "    ax.imshow(image)\n",
    "    # ...with detections\n",
    "    bboxes, classes, confidences = best_results_per_input[image_idx]\n",
    "    for idx in range(len(bboxes)):\n",
    "        left, bot, right, top = bboxes[idx]\n",
    "        x, y, w, h = [val * 300 for val in [left, bot, right - left, top - bot]]\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, \"{} {:.0f}%\".format(classes_to_labels[classes[idx] - 1], confidences[idx]*100), bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-distributor",
   "metadata": {
    "id": "personal-distributor"
   },
   "source": [
    "### References\n",
    "\n",
    " - [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) paper\n",
    " - [Speed/accuracy trade-offs for modern convolutional object detectors](https://arxiv.org/abs/1611.10012) paper\n",
    " - [SSD on NGC](https://ngc.nvidia.com/catalog/resources/nvidia:ssd_for_pytorch)\n",
    " - [SSD on github](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
